{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.2.0\n",
      "Torchvision Version:  0.4.0a0+6b959ee\n",
      "Using GPU!\n"
     ]
    }
   ],
   "source": [
    "import custom_models\n",
    "#python packages\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm\n",
    "import gc\n",
    "import datetime\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from skimage import io\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "#torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU!\")\n",
    "else:\n",
    "    print(\"WARNING: Could not find GPU! Using CPU only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_CNN(model_name, num_classes, resume_from = None):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    # The model (nn.Module) to return\n",
    "    model_ft = None\n",
    "    # The input image is expected to be (input_size, input_size)\n",
    "    input_size = 0\n",
    "    \n",
    "    # You may NOT use pretrained models!! \n",
    "    use_pretrained = False\n",
    "    \n",
    "    # By default, all parameters will be trained (useful when you're starting from scratch)\n",
    "    # Within this function you can set .requires_grad = False for various parameters, if you\n",
    "    # don't want to learn them\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    elif model_name == \"vgg13\":\n",
    "        \"\"\" VGG13_bn\n",
    "        \"\"\"\n",
    "        model_ft = custom_models.vgg13_bn(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    elif model_name == \"vgg16\":\n",
    "        \"\"\" VGG16_bn\n",
    "        \"\"\"\n",
    "        model_ft = custom_models.vgg13_bn(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    elif model_name == \"vgg19\":\n",
    "        \"\"\" VGG19_bn\n",
    "        \"\"\"\n",
    "        model_ft = custom_models.vgg13_bn(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "        input_size = 224\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Invalid model name!\")\n",
    "    \n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chest_Disease_Net(nn.Module):\n",
    "    \"\"\"\n",
    "    fc1: number of neurons in the hidden fully connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, cnn_model_name, num_classes, num_multimodal_features=12, fc1_out=32, resume_from=None):\n",
    "        #num_classes = 14\n",
    "        #num_multimodal_features= 12\n",
    "        super(Chest_Disease_Net, self).__init__()\n",
    "        self.cnn, self.input_size = make_CNN(cnn_model_name, num_classes)#models.vgg11(pretrained=False, progress = True)\n",
    "        #define output layers\n",
    "        self.fc1 = nn.Linear(num_classes + num_multimodal_features, fc1_out) #takes in input of CNN and multimodal input\n",
    "        self.fc2 = nn.Linear(fc1_out, num_classes)\n",
    "        if resume_from is not None:\n",
    "            print(\"Loading weights from %s\" % resume_from)\n",
    "            self.load_state_dict(torch.load(resume_from))\n",
    "        \n",
    "    def forward(self, image, data):\n",
    "        x1 = self.cnn(image)\n",
    "        x2 = data \n",
    "        x = torch.cat((x1.float(), x2.float()), dim=1) ### ???\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x.double() ### ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset definition\n",
    "    \"\"\"\n",
    "    def __init__(self, pandas_dataframe, img_path, transform=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.df = pandas_dataframe\n",
    "        self.img_path = img_path\n",
    "        self.transform = transform\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        img_name = self.df.iloc[index][\"img_name\"] \n",
    "        #print(img_name)\n",
    "        img_path = os.path.join(self.img_path, img_name)\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = np.asarray(image)\n",
    "        #image = io.imread(image, as_gray = True)\n",
    "\n",
    "        #print(\"Image shape: \", image.shape)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor # ???\n",
    "        features = np.fromstring(self.df.iloc[index][\"feature\"][1:-1], sep=\",\") # ???\n",
    "        features = torch.from_numpy(features.astype(\"float\")) # ???\n",
    "        label = int(self.df.iloc[index]['label'])\n",
    "        \n",
    "        #print(\"Label type: \", type(label))\n",
    "        #label = np.int_(label) #???\n",
    "        #print(\"label type post casting: \", type(label))\n",
    "        return image, features, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(provided_df, input_size, batch_size):\n",
    "    # How to transform the image when you are loading them.\n",
    "    # you'll likely want to mess with the transforms on the training set.\n",
    "    \n",
    "    # For now, we resize/crop the image to the correct input size for our network,\n",
    "    # then convert it to a [C,H,W] tensor, then normalize it to values with a given mean/stdev. These normalization constants\n",
    "    # are derived from aggregating lots of data and happen to produce better results.\n",
    "    data_transforms = {\n",
    "        'test': transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.225])\n",
    "        ])\n",
    "    }\n",
    "    # Create training and validation datasets\n",
    "    data_set = MultimodalDataset(pandas_dataframe = provided_df, \n",
    "                                         img_path=\"/storage/images\", \n",
    "                                         transform=data_transforms[\"test\"])\n",
    "    dataloaders_dict = DataLoader(data_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    return dataloaders_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    model_name = \"vgg\"\n",
    "    num_classes = 14\n",
    "    model = Chest_Disease_Net(cnn_model_name = model_name, num_classes = num_classes, resume_from = None)\n",
    "    model_path = path\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_disease(path): \n",
    "    df = pd.read_csv(\"./data/test_dataset.csv\").drop(['Unnamed: 0', 'Unnamed: 0.1', 'dataset_type', 'disease'],axis=1) #test_dataset\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        \n",
    "    disease_names = ['pneumothorax', 'pneumonia', 'pleuralthickening', 'nofinding', 'nodule', 'mass', 'infiltration', 'fibrosis', 'emphysema', 'effusion', 'edema', 'consolidation', 'cardiomegaly', 'atelectasis']\n",
    "    disease_to_label = {}\n",
    "    for index,value in enumerate(disease_names):\n",
    "        disease_to_label[index] = value\n",
    "\n",
    "      \n",
    "    input_size = 256\n",
    "    batch_size = 64\n",
    "    \n",
    "    \n",
    "    model = load_model(path)    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(14):\n",
    "        new_df = df[df[\"label\"] == float(i)]\n",
    "        dataloader = get_dataloaders(new_df, input_size, batch_size)\n",
    "        correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(dataloader):\n",
    "                images, features, labels = data\n",
    "                images = images.to(device)\n",
    "                features = feature.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images, features)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                test_total += labels.size(0)\n",
    "\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Top One Error of the network on test images for' + str(disease_to_label[i]) + ': %d %%' % (\n",
    "                100 * (1 - correct / test_total)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Chest_Disease_Net:\n\tMissing key(s) in state_dict: \"cnn.features.0.weight\", \"cnn.features.0.bias\", \"cnn.features.1.weight\", \"cnn.features.1.bias\", \"cnn.features.1.running_mean\", \"cnn.features.1.running_var\", \"cnn.features.4.weight\", \"cnn.features.4.bias\", \"cnn.features.5.weight\", \"cnn.features.5.bias\", \"cnn.features.5.running_mean\", \"cnn.features.5.running_var\", \"cnn.features.8.weight\", \"cnn.features.8.bias\", \"cnn.features.9.weight\", \"cnn.features.9.bias\", \"cnn.features.9.running_mean\", \"cnn.features.9.running_var\", \"cnn.features.11.weight\", \"cnn.features.11.bias\", \"cnn.features.12.weight\", \"cnn.features.12.bias\", \"cnn.features.12.running_mean\", \"cnn.features.12.running_var\", \"cnn.features.15.weight\", \"cnn.features.15.bias\", \"cnn.features.16.weight\", \"cnn.features.16.bias\", \"cnn.features.16.running_mean\", \"cnn.features.16.running_var\", \"cnn.features.18.weight\", \"cnn.features.18.bias\", \"cnn.features.19.weight\", \"cnn.features.19.bias\", \"cnn.features.19.running_mean\", \"cnn.features.19.running_var\", \"cnn.features.22.weight\", \"cnn.features.22.bias\", \"cnn.features.23.weight\", \"cnn.features.23.bias\", \"cnn.features.23.running_mean\", \"cnn.features.23.running_var\", \"cnn.features.25.weight\", \"cnn.features.25.bias\", \"cnn.features.26.weight\", \"cnn.features.26.bias\", \"cnn.features.26.running_mean\", \"cnn.features.26.running_var\", \"cnn.classifier.0.weight\", \"cnn.classifier.0.bias\", \"cnn.classifier.3.weight\", \"cnn.classifier.3.bias\", \"cnn.classifier.6.weight\", \"cnn.classifier.6.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"layer1.0.weight\", \"layer1.0.bias\", \"layer2.0.weight\", \"layer2.0.bias\", \"layer3.0.weight\", \"layer3.0.bias\", \"layer4.0.weight\", \"layer4.0.bias\", \"layer5.0.weight\", \"layer5.0.bias\", \"fc3.weight\", \"fc3.bias\", \"fc1.0.weight\", \"fc1.0.bias\", \"fc2.0.weight\", \"fc2.0.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9e7f10cd05df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/ubuntu/6.867-xray-project/weights/model.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mper_disease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-359667ead003>\u001b[0m in \u001b[0;36mper_disease\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-38d0817006fa>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 845\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Chest_Disease_Net:\n\tMissing key(s) in state_dict: \"cnn.features.0.weight\", \"cnn.features.0.bias\", \"cnn.features.1.weight\", \"cnn.features.1.bias\", \"cnn.features.1.running_mean\", \"cnn.features.1.running_var\", \"cnn.features.4.weight\", \"cnn.features.4.bias\", \"cnn.features.5.weight\", \"cnn.features.5.bias\", \"cnn.features.5.running_mean\", \"cnn.features.5.running_var\", \"cnn.features.8.weight\", \"cnn.features.8.bias\", \"cnn.features.9.weight\", \"cnn.features.9.bias\", \"cnn.features.9.running_mean\", \"cnn.features.9.running_var\", \"cnn.features.11.weight\", \"cnn.features.11.bias\", \"cnn.features.12.weight\", \"cnn.features.12.bias\", \"cnn.features.12.running_mean\", \"cnn.features.12.running_var\", \"cnn.features.15.weight\", \"cnn.features.15.bias\", \"cnn.features.16.weight\", \"cnn.features.16.bias\", \"cnn.features.16.running_mean\", \"cnn.features.16.running_var\", \"cnn.features.18.weight\", \"cnn.features.18.bias\", \"cnn.features.19.weight\", \"cnn.features.19.bias\", \"cnn.features.19.running_mean\", \"cnn.features.19.running_var\", \"cnn.features.22.weight\", \"cnn.features.22.bias\", \"cnn.features.23.weight\", \"cnn.features.23.bias\", \"cnn.features.23.running_mean\", \"cnn.features.23.running_var\", \"cnn.features.25.weight\", \"cnn.features.25.bias\", \"cnn.features.26.weight\", \"cnn.features.26.bias\", \"cnn.features.26.running_mean\", \"cnn.features.26.running_var\", \"cnn.classifier.0.weight\", \"cnn.classifier.0.bias\", \"cnn.classifier.3.weight\", \"cnn.classifier.3.bias\", \"cnn.classifier.6.weight\", \"cnn.classifier.6.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"layer1.0.weight\", \"layer1.0.bias\", \"layer2.0.weight\", \"layer2.0.bias\", \"layer3.0.weight\", \"layer3.0.bias\", \"layer4.0.weight\", \"layer4.0.bias\", \"layer5.0.weight\", \"layer5.0.bias\", \"fc3.weight\", \"fc3.bias\", \"fc1.0.weight\", \"fc1.0.bias\", \"fc2.0.weight\", \"fc2.0.bias\". "
     ]
    }
   ],
   "source": [
    "file_path = \"/home/ubuntu/6.867-xray-project/weights/model.0\"\n",
    "per_disease(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/home/ubuntu/6.867-xray-project/weights/model.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['layer1.0.weight', 'layer1.0.bias', 'layer2.0.weight', 'layer2.0.bias', 'layer3.0.weight', 'layer3.0.bias', 'layer4.0.weight', 'layer4.0.bias', 'layer5.0.weight', 'layer5.0.bias', 'fc1.0.weight', 'fc1.0.bias', 'fc2.0.weight', 'fc2.0.bias', 'fc3.weight', 'fc3.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
