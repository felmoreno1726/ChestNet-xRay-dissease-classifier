{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.2.0\n",
      "Torchvision Version:  0.4.0a0+6b959ee\n",
      "Using GPU!\n"
     ]
    }
   ],
   "source": [
    "import custom_models\n",
    "#python packages\n",
    "from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm\n",
    "import gc\n",
    "import datetime\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from skimage import io\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "#torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU!\")\n",
    "else:\n",
    "    print(\"WARNING: Could not find GPU! Using CPU only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_CNN(model_name, num_classes, resume_from = None):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    # The model (nn.Module) to return\n",
    "    model_ft = None\n",
    "    # The input image is expected to be (input_size, input_size)\n",
    "    input_size = 0\n",
    "    \n",
    "    # You may NOT use pretrained models!! \n",
    "    use_pretrained = False\n",
    "    \n",
    "    # By default, all parameters will be trained (useful when you're starting from scratch)\n",
    "    # Within this function you can set .requires_grad = False for various parameters, if you\n",
    "    # don't want to learn them\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = custom_models.vgg11_bn(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    elif model_name == \"vgg13\":\n",
    "        \"\"\" VGG13_bn\n",
    "        \"\"\"\n",
    "        model_ft = custom_models.vgg13_bn(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    elif model_name == \"vgg16\":\n",
    "        \"\"\" VGG16_bn\n",
    "        \"\"\"\n",
    "        model_ft = custom_models.vgg13_bn(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    elif model_name == \"vgg19\":\n",
    "        \"\"\" VGG19_bn\n",
    "        \"\"\"\n",
    "        model_ft = custom_models.vgg13_bn(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "        input_size = 224\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Invalid model name!\")\n",
    "    \n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Multimodal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chest_Disease_Net(nn.Module):\n",
    "    \"\"\"\n",
    "    fc1: number of neurons in the hidden fully connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, cnn_model_name, num_classes, num_multimodal_features=12, fc1_out=32, resume_from=None):\n",
    "        #num_classes = 14\n",
    "        #num_multimodal_features= 12\n",
    "        super(Chest_Disease_Net, self).__init__()\n",
    "        self.cnn, self.input_size = make_CNN(cnn_model_name, num_classes)#models.vgg11(pretrained=False, progress = True)\n",
    "        #define output layers\n",
    "        self.fc1 = nn.Linear(num_classes + num_multimodal_features, fc1_out) #takes in input of CNN and multimodal input\n",
    "        self.fc2 = nn.Linear(fc1_out, num_classes)\n",
    "        if resume_from is not None:\n",
    "            print(\"Loading weights from %s\" % resume_from)\n",
    "            self.load_state_dict(torch.load(resume_from))\n",
    "        \n",
    "    def forward(self, image, data):\n",
    "        x1 = self.cnn(image)\n",
    "        x2 = data\n",
    "        print(\"x1: \", x1, type(x1))\n",
    "        print(\"x2: \", x2, type(x2))\n",
    "        #x = torch.cat((x1, x2), dim=1)  \n",
    "        x = torch.cat((x1.float(), x2.float()), dim=1) ### ???\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x.double() ### ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset definition\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path, img_path, transform=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.img_path = img_path\n",
    "        self.transform = transform\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        img_name = self.df.iloc[index][\"img_name\"] \n",
    "        img_path = os.path.join(self.img_path, img_name)\n",
    "        image = io.imread(img_path)\n",
    "        #print(\"Image shape: \", image.shape)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor # ???\n",
    "        features = np.fromstring(self.df.iloc[index][\"feature\"][1:-1], sep=\",\") # ???\n",
    "        features = torch.from_numpy(features.astype(\"float\")) # ???\n",
    "        label = self.df.iloc[index]['label']\n",
    "        #print(\"Label type: \", type(label))\n",
    "        #label = np.int_(label) #???\n",
    "        #print(\"label type post casting: \", type(label))\n",
    "        return image, features, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(input_size, batch_size, augment=False, shuffle = True):\n",
    "    # How to transform the image when you are loading them.\n",
    "    # you'll likely want to mess with the transforms on the training set.\n",
    "    \n",
    "    # For now, we resize/crop the image to the correct input size for our network,\n",
    "    # then convert it to a [C,H,W] tensor, then normalize it to values with a given mean/stdev. These normalization constants\n",
    "    # are derived from aggregating lots of data and happen to produce better results.\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            #Add extra transformations for data augmentation\n",
    "            transforms.RandomApply([\n",
    "                transforms.RandomChoice([\n",
    "                    transforms.RandomAffine(degrees=20),\n",
    "                    transforms.RandomAffine(degrees=0,scale=(0.1, 0.15)),\n",
    "                    transforms.RandomAffine(degrees=0,translate=(0.2,0.2)),\n",
    "                    #transforms.RandomAffine(degrees=0,shear=0.15),\n",
    "                    transforms.RandomHorizontalFlip(p=1.0)\n",
    "                ] if augment else [transforms.RandomAffine(degrees=0)])#else do nothing\n",
    "            ], p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.225])\n",
    "        ])\n",
    "    }\n",
    "    # Create training and validation datasets\n",
    "    data_subsets = {x: MultimodalDataset(csv_path=\"./data/\"+x+\"_dataset.csv\", \n",
    "                                         img_path=\"/storage/images\", \n",
    "                                         transform=data_transforms[x]) for x in data_transforms.keys()}\n",
    "    # Create training and validation dataloaders\n",
    "    # Never shuffle the test set\n",
    "    dataloaders_dict = {x: DataLoader(data_subsets[x], batch_size=batch_size, shuffle=False if x != 'train' else shuffle, num_workers=4) for x in data_transforms.keys()}\n",
    "    return dataloaders_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, model_name=str(datetime.datetime.now()), \n",
    "                save_dir = None, save_all_epochs=False, num_epochs=25):\n",
    "    '''\n",
    "    model: The NN to train\n",
    "    dataloaders: A dictionary containing at least the keys \n",
    "                 'train','val' that maps to Pytorch data loaders for the dataset\n",
    "    criterion: The Loss function\n",
    "    optimizer: The algorithm to update weights \n",
    "               (Variations on gradient descent)\n",
    "    num_epochs: How many epochs to train for\n",
    "    save_dir: Where to save the best model weights that are found, \n",
    "              as they are found. Will save to save_dir/weights_best.pt\n",
    "              Using None will not write anything to disk\n",
    "    save_all_epochs: Whether to save weights for ALL epochs, not just the best\n",
    "                     validation error epoch. Will save to save_dir/weights_e{#}.pt\n",
    "    '''\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            # TQDM has nice progress bars\n",
    "            for inputs, features, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                features = features.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs, features)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # torch.max outputs the maximum value, and its index\n",
    "                    # Since the input is batched, we take the max along axis 1\n",
    "                    # (the meaningful outputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backprop + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), save_dir + \"/{}_best_weights.pt\".format(model_name))\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "        print()\n",
    "        scheduler.step()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(model):\n",
    "    # Get all the parameters\n",
    "    params_to_update = model.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "    # Use SGD\n",
    "    optimizer = optim.SGD(params_to_update, lr=0.01, momentum=0.9)\n",
    "    return optimizer\n",
    "\n",
    "def get_loss():\n",
    "    # Create an instance of the loss function\n",
    "    criterion = nn.CrossEntropyLoss()#TODO set weights to account for unbalanced data (50% is non-diseased)\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet]\n",
    "# You can add your own, or modify these however you wish!\n",
    "model_name = \"vgg\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "# Miniplaces has 100\n",
    "num_classes = 14\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "# You should use a power of 2.\n",
    "batch_size = 64\n",
    "\n",
    "# Shuffle the input data?\n",
    "shuffle_datasets = True\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 10\n",
    "\n",
    "### IO\n",
    "# Path to a model file to use to start weights at\n",
    "#resume_from = \"/home/ubuntu/6.867-xray-project/weights/data_aug_vgg.pt\"\n",
    "resume_from = None\n",
    "\n",
    "# Directory to save weights to\n",
    "save_dir = \"weights\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# If True saves the weights for all epochs, else only saves the weight of best one\n",
    "save_all_epochs = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t cnn.features.0.weight\n",
      "\t cnn.features.0.bias\n",
      "\t cnn.features.1.weight\n",
      "\t cnn.features.1.bias\n",
      "\t cnn.features.4.weight\n",
      "\t cnn.features.4.bias\n",
      "\t cnn.features.5.weight\n",
      "\t cnn.features.5.bias\n",
      "\t cnn.features.8.weight\n",
      "\t cnn.features.8.bias\n",
      "\t cnn.features.9.weight\n",
      "\t cnn.features.9.bias\n",
      "\t cnn.features.11.weight\n",
      "\t cnn.features.11.bias\n",
      "\t cnn.features.12.weight\n",
      "\t cnn.features.12.bias\n",
      "\t cnn.features.15.weight\n",
      "\t cnn.features.15.bias\n",
      "\t cnn.features.16.weight\n",
      "\t cnn.features.16.bias\n",
      "\t cnn.features.18.weight\n",
      "\t cnn.features.18.bias\n",
      "\t cnn.features.19.weight\n",
      "\t cnn.features.19.bias\n",
      "\t cnn.features.22.weight\n",
      "\t cnn.features.22.bias\n",
      "\t cnn.features.23.weight\n",
      "\t cnn.features.23.bias\n",
      "\t cnn.features.25.weight\n",
      "\t cnn.features.25.bias\n",
      "\t cnn.features.26.weight\n",
      "\t cnn.features.26.bias\n",
      "\t cnn.classifier.0.weight\n",
      "\t cnn.classifier.0.bias\n",
      "\t cnn.classifier.3.weight\n",
      "\t cnn.classifier.3.bias\n",
      "\t cnn.classifier.6.weight\n",
      "\t cnn.classifier.6.bias\n",
      "\t fc1.weight\n",
      "\t fc1.bias\n",
      "\t fc2.weight\n",
      "\t fc2.bias\n",
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1621f22233bf43108ba8fcfeac09e172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1325), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1:  tensor([[-6.9547e-01, -7.5157e-01, -2.8253e-01,  5.1226e-01, -1.3988e-03,\n",
      "         -2.3172e-01,  7.9518e-01,  4.6461e-01,  3.1393e-01,  1.5958e-01,\n",
      "          7.6475e-01, -3.3412e-01, -3.9352e-01,  1.1314e+00],\n",
      "        [ 6.0650e-01, -4.5188e-01, -6.1918e-02,  2.9675e-01,  2.3418e-01,\n",
      "         -4.8306e-01,  2.2920e-02, -4.6251e-01,  8.6921e-01, -8.4334e-01,\n",
      "         -6.1922e-01, -9.8837e-01, -7.7013e-01,  8.5822e-01],\n",
      "        [ 6.2251e-01, -1.3888e+00,  5.6442e-01,  8.0405e-01,  8.1662e-01,\n",
      "          1.3781e+00, -1.5176e+00,  9.4178e-01,  1.1152e+00,  5.9746e-03,\n",
      "          3.4687e-01, -8.6746e-02, -8.5248e-01,  4.6986e-01],\n",
      "        [-3.4919e-01, -2.1813e-01,  1.9805e-01,  8.5352e-01, -1.0275e-02,\n",
      "          2.0327e-02, -1.0287e+00,  3.5220e-01,  7.4395e-01,  4.1003e-02,\n",
      "          2.5674e-01,  4.6895e-02, -5.3083e-02,  1.0736e+00],\n",
      "        [ 1.8000e-01, -9.9196e-02, -7.3953e-02,  5.9085e-01, -4.3875e-01,\n",
      "          1.0327e-01,  5.7129e-01,  2.6148e-01,  6.8045e-01,  5.2683e-01,\n",
      "          2.0876e-01, -6.6770e-01, -9.3893e-01,  2.0141e-01],\n",
      "        [-7.8598e-02,  1.2967e-01, -2.9378e-01,  6.5422e-01, -5.0379e-01,\n",
      "         -9.8626e-01, -6.5021e-01, -2.6381e-01,  9.4693e-01, -6.2735e-01,\n",
      "          9.5244e-01, -9.4944e-01, -3.4233e-01,  7.6439e-01],\n",
      "        [-1.8829e-02,  3.3504e-01, -2.4929e-01,  9.7445e-02, -6.7504e-01,\n",
      "          8.7906e-02, -8.5810e-02,  3.3746e-01,  1.3993e+00,  6.0834e-01,\n",
      "          3.7279e-01, -1.1951e+00, -1.1973e+00,  3.6422e-01],\n",
      "        [-3.4327e-01,  1.6602e-01,  2.0133e-01,  1.3719e+00, -4.9126e-01,\n",
      "          1.2665e-01,  7.1182e-01, -2.9158e-01, -1.0958e-01,  1.0971e-01,\n",
      "         -3.3285e-01, -1.1617e+00, -3.1442e-01,  1.0582e+00],\n",
      "        [-1.6081e-02, -1.8165e+00, -6.1863e-01,  7.5286e-01, -9.5469e-03,\n",
      "          2.5189e-01,  8.3941e-01, -1.5162e-01,  3.3173e-01,  3.8608e-01,\n",
      "          9.8396e-02, -1.2882e+00, -5.4512e-01,  6.6671e-01],\n",
      "        [ 4.5147e-01, -8.7401e-01, -7.5505e-01,  1.4530e-01, -2.7792e-01,\n",
      "         -1.4541e+00,  5.6300e-01, -6.8001e-01,  1.4624e+00,  7.5528e-01,\n",
      "         -1.0842e-01, -1.2964e-01, -7.5942e-01,  9.7355e-01],\n",
      "        [ 4.4853e-01, -1.5475e-01,  2.6560e-01,  1.1230e-01, -4.0918e-01,\n",
      "         -3.3716e-01,  5.0998e-01,  4.0221e-01,  5.0023e-01, -8.0392e-01,\n",
      "         -8.5836e-03, -4.0257e-01, -7.7868e-01,  4.3911e-01],\n",
      "        [ 3.4555e-01, -7.2062e-01,  7.8256e-01,  2.1803e+00,  2.1227e-01,\n",
      "          4.0702e-01, -5.2546e-01, -7.3436e-01,  1.0627e+00,  5.7919e-01,\n",
      "          4.4711e-01, -4.5004e-01, -9.7115e-01,  2.1487e-01],\n",
      "        [-6.4502e-01,  9.2700e-01, -2.6132e-01,  3.1971e-01,  6.9592e-01,\n",
      "          1.2941e+00,  7.2198e-01, -1.4566e+00, -4.9883e-01,  1.3476e-01,\n",
      "          8.3637e-01,  1.1694e-01, -9.9383e-01,  1.4323e+00],\n",
      "        [ 1.6171e-01,  4.6984e-01,  4.6975e-01,  4.8452e-01,  2.5919e-01,\n",
      "         -9.4436e-01,  5.7766e-01, -1.1735e+00,  1.6655e-01,  3.3279e-01,\n",
      "         -2.0266e-01, -1.7714e+00,  1.5799e-02,  9.5302e-01],\n",
      "        [-7.6672e-01, -5.3964e-01, -1.0174e+00, -1.1909e+00, -2.0414e-01,\n",
      "         -3.0163e-01, -1.0127e+00, -5.7233e-01,  1.2957e+00, -2.3619e-01,\n",
      "         -2.2100e-01, -2.0181e+00,  2.8246e-01,  1.0844e+00],\n",
      "        [ 1.5408e-01, -1.1210e+00, -1.3529e-02, -4.0715e-01,  2.4875e-01,\n",
      "          2.1157e-01, -3.6985e-01, -2.5711e-01,  4.6059e-01, -3.5365e-02,\n",
      "          3.3530e-01, -7.9324e-01, -9.8103e-01,  8.0321e-03],\n",
      "        [ 1.0860e-01,  6.6271e-02, -2.5673e-01,  3.1704e-01,  1.6153e-01,\n",
      "          3.4010e-02,  1.3561e-01, -1.9121e-01,  7.1864e-01, -1.6232e-01,\n",
      "          1.8769e-01, -1.0457e+00,  8.5293e-02,  3.8744e-01],\n",
      "        [-1.5084e-01,  4.0603e-01,  6.4311e-01,  2.3617e-01, -4.0637e-01,\n",
      "          4.5958e-02,  8.0642e-01, -7.9329e-01,  2.5192e-01, -2.6849e-02,\n",
      "          3.6021e-01, -1.5224e+00,  9.8374e-02,  1.0033e+00],\n",
      "        [-9.4790e-02, -3.7064e-01, -1.9631e-01,  3.3093e-02, -8.8807e-01,\n",
      "          1.6988e-01, -2.8315e-01,  8.6882e-02,  5.2537e-01,  5.8045e-01,\n",
      "         -6.1824e-01, -1.0463e+00,  2.6382e-01,  6.0703e-01],\n",
      "        [-2.5543e-01,  1.6455e-01, -5.5918e-01,  3.0857e-01, -1.3490e-01,\n",
      "         -8.9072e-02,  2.4104e-01,  7.4015e-01,  1.3145e+00,  1.4454e-01,\n",
      "          1.8839e-01, -6.7879e-01,  2.5582e-01, -1.1291e-01],\n",
      "        [-4.8044e-01,  2.3347e-01, -2.7058e-01,  9.6719e-01, -6.2383e-01,\n",
      "          4.9586e-01, -2.2269e-02, -3.9867e-01,  8.1359e-01,  4.2167e-01,\n",
      "          1.1948e-01, -5.5148e-01,  6.0036e-01,  5.0209e-01],\n",
      "        [-1.6162e-02, -2.7234e-01, -2.1680e-01,  4.5875e-01, -6.0759e-01,\n",
      "          3.9320e-01, -1.3017e-01, -2.1059e-01,  3.1294e-01, -2.3256e-01,\n",
      "          5.3918e-01, -5.5094e-01, -6.0124e-01, -5.7626e-02],\n",
      "        [ 2.0819e-01, -2.1104e-01, -6.6319e-01, -6.3287e-01, -1.7795e+00,\n",
      "          5.0069e-01, -5.0104e-01,  3.5061e-01,  2.2293e+00,  1.3334e+00,\n",
      "          1.6372e+00, -5.5360e-01,  7.1169e-01,  7.7333e-01],\n",
      "        [-1.5287e+00, -1.6429e-01, -1.4053e+00,  8.2514e-01,  2.1078e-01,\n",
      "          2.0192e+00,  8.2084e-02,  1.1829e+00,  1.0328e+00,  3.4681e-01,\n",
      "          1.7182e+00, -2.8619e-01, -5.5178e-01,  2.1894e+00],\n",
      "        [-2.6349e-01, -7.0022e-01, -3.3910e-01, -5.1757e-01,  3.8962e-01,\n",
      "          8.7082e-01, -2.9100e-01,  4.9411e-01,  6.0165e-01,  9.3715e-01,\n",
      "         -3.2255e-01, -4.7721e-01, -4.1492e-01,  1.1188e+00],\n",
      "        [ 6.2649e-01,  4.4175e-01,  4.6272e-01,  1.1608e+00, -2.9360e-01,\n",
      "          1.0012e-01, -5.3707e-01, -6.2437e-01,  8.0919e-01, -8.0993e-01,\n",
      "          7.0332e-01, -1.4754e+00, -2.8412e-01,  2.6142e+00],\n",
      "        [-1.7650e-01,  8.7244e-01, -4.2326e-01,  4.2545e-01, -3.5297e-01,\n",
      "          5.5157e-02, -5.3853e-01,  1.2205e-01, -2.9286e-01,  3.2774e-01,\n",
      "         -1.0986e+00, -2.2503e-01, -1.0889e+00,  3.6547e-01],\n",
      "        [ 6.5427e-01,  1.4097e-01,  2.0033e-01, -1.0113e-01, -2.0293e+00,\n",
      "          1.1246e+00,  1.1727e+00,  1.1001e+00,  7.0944e-01,  9.8159e-02,\n",
      "          1.8927e-01, -9.7109e-01, -1.2724e+00,  2.1759e+00],\n",
      "        [-4.5946e-01, -8.2015e-01, -4.9934e-01,  1.4993e+00,  1.1204e+00,\n",
      "          7.1790e-01, -5.2021e-02, -3.1926e-01, -9.5559e-01,  4.5974e-01,\n",
      "          8.6660e-01,  6.2459e-02, -5.8020e-01,  6.2632e-01],\n",
      "        [ 9.9741e-02,  1.1823e+00,  7.6783e-01,  3.2546e-01, -7.6257e-02,\n",
      "         -1.1351e-01, -1.0917e-01,  4.4804e-01,  1.2066e+00,  3.2272e-02,\n",
      "          8.2789e-02,  1.6175e-01,  3.7082e-02,  2.8446e-01],\n",
      "        [-1.1029e+00, -1.4732e+00, -4.1695e-01,  4.8103e-01, -1.0338e+00,\n",
      "          3.3167e-01, -5.5604e-02, -9.1219e-02,  1.5246e+00, -6.6700e-01,\n",
      "          9.5536e-01, -1.5632e+00, -6.9030e-02,  8.4855e-01],\n",
      "        [-2.7436e-01, -2.4693e-01,  1.5991e-01, -6.0144e-01,  2.5119e-01,\n",
      "         -5.1714e-01, -2.4236e-01,  8.8922e-01,  2.3865e+00, -5.3229e-01,\n",
      "         -5.2529e-01, -1.1946e+00, -7.3903e-01,  5.4207e-01],\n",
      "        [-3.2475e-01,  8.3272e-01, -2.2770e-01, -1.8518e-02, -2.4630e-01,\n",
      "         -7.7892e-02, -8.0252e-02,  6.0252e-01,  3.9729e-01,  3.9563e-01,\n",
      "         -2.5309e-01,  1.0507e-01,  1.5699e-01, -4.3332e-01],\n",
      "        [ 5.5908e-01,  2.4813e-01, -3.4504e-01, -7.6200e-01, -1.4510e+00,\n",
      "          1.1762e+00,  3.0428e-01, -8.1627e-01,  4.0892e-01,  6.2513e-01,\n",
      "         -6.3442e-01,  4.9940e-01,  1.2313e+00,  1.4447e+00],\n",
      "        [-2.3711e-01, -1.5541e-01, -4.9451e-01,  2.2596e-01,  1.8089e-01,\n",
      "          4.3001e-01, -1.0558e+00,  4.6737e-01,  1.0525e-02, -1.1277e-01,\n",
      "         -1.7816e-01, -1.6523e+00, -2.0996e-01,  1.6573e-01],\n",
      "        [ 1.0149e+00,  9.4458e-01,  1.4145e-01,  2.6979e-01, -2.3650e+00,\n",
      "          6.5046e-01,  5.1612e-01, -3.7537e-01,  9.5387e-01,  1.0246e+00,\n",
      "          1.0198e+00, -5.0372e-01, -9.3819e-01,  6.1915e-01],\n",
      "        [-4.6747e-02,  7.9626e-01, -9.8481e-02,  5.6138e-02,  8.7149e-02,\n",
      "          1.4212e-01, -6.4137e-02,  8.5651e-03,  6.3805e-01,  9.8040e-01,\n",
      "         -2.4503e-02, -1.1415e+00,  1.9954e-01, -1.2323e+00],\n",
      "        [-4.0753e-01,  1.3732e+00,  7.6219e-01,  2.9695e-01, -2.9334e-01,\n",
      "          1.0582e+00,  1.4731e+00,  9.3410e-01,  1.8128e+00,  2.8974e-01,\n",
      "         -5.3169e-02, -1.4212e+00, -5.8592e-01,  1.4344e+00],\n",
      "        [-4.9753e-02, -1.4067e+00, -1.1114e+00,  4.3260e-01, -1.1758e+00,\n",
      "          7.0308e-01,  8.5283e-02,  5.2574e-01,  1.3600e+00, -5.1882e-02,\n",
      "          7.1236e-01, -1.2590e+00,  1.0194e+00,  1.4328e-01],\n",
      "        [-5.1791e-01, -5.8252e-02, -2.1610e-01,  8.8063e-01, -2.3539e-01,\n",
      "         -4.5976e-01,  6.6181e-01, -3.0483e-01,  8.5120e-01,  4.0254e-01,\n",
      "          5.4874e-01, -6.2282e-01, -3.2828e-01,  4.0747e-01],\n",
      "        [-8.3101e-03, -5.1679e-01,  1.7997e-01, -2.2421e-02, -1.0795e-01,\n",
      "          4.4467e-01, -5.0032e-01,  2.4978e-01,  5.7489e-01,  5.4345e-01,\n",
      "          7.2253e-01, -5.0412e-01,  4.1404e-01,  6.9043e-01],\n",
      "        [-2.4860e-01, -8.0291e-01, -1.3869e-01,  3.5293e-01,  4.3415e-01,\n",
      "          1.2367e-02,  1.0085e+00, -1.1858e-01,  1.8080e-01,  4.0891e-01,\n",
      "         -5.3211e-01, -1.2072e-02, -1.4571e-01,  6.1049e-01],\n",
      "        [-6.3927e-01, -7.4214e-01, -3.0275e-01,  2.9934e-01, -3.2363e-01,\n",
      "         -1.1677e-01,  3.6317e-02, -2.5761e-02,  9.1864e-01,  6.2177e-01,\n",
      "         -6.4906e-01, -8.5448e-01,  1.0406e-01,  1.1178e-01],\n",
      "        [-1.4995e-01,  6.7962e-01,  3.1771e-01,  1.1859e-01, -4.4264e-01,\n",
      "          2.4650e-01,  1.5749e-01, -1.4055e-01,  5.0184e-01,  7.5639e-01,\n",
      "          3.4049e-01, -5.0715e-01, -8.2275e-01,  1.1195e-01],\n",
      "        [ 3.9806e-02, -4.2558e-03, -8.5098e-01,  4.2476e-01,  3.6657e-02,\n",
      "         -2.2195e-02, -2.8304e-01, -4.0769e-01,  8.1385e-01,  7.3248e-01,\n",
      "          5.4307e-01, -5.5429e-01, -6.4911e-01,  8.1941e-02],\n",
      "        [ 3.4967e-01,  7.1292e-01, -1.1613e+00, -9.2513e-02,  3.7776e-01,\n",
      "          7.3123e-01,  6.2232e-01,  4.3569e-01,  9.8370e-01,  6.2760e-01,\n",
      "          3.2021e-01, -4.2353e-01, -7.7953e-01,  4.7832e-01],\n",
      "        [ 8.8170e-04,  1.3081e-02, -3.2641e-01,  1.0329e+00,  3.2133e-02,\n",
      "          1.6436e-01,  3.0396e-01, -9.4721e-01,  1.0296e+00,  2.3588e-01,\n",
      "         -1.4387e-01, -1.0161e+00,  3.4571e-02,  2.0441e-01],\n",
      "        [ 1.5582e-01, -2.8278e-01, -4.7029e-02,  5.5753e-01,  1.0573e+00,\n",
      "         -3.7710e-01, -1.6457e-01,  3.5196e-01,  8.0295e-01,  5.6897e-01,\n",
      "          1.4148e-01,  1.9435e-01,  2.2973e-01,  4.9282e-01],\n",
      "        [ 1.8606e-01,  2.9012e-01,  6.4507e-01,  1.2043e+00, -2.3826e-01,\n",
      "         -2.5849e-01, -8.1190e-01,  1.3382e-01, -7.1660e-01, -4.3484e-01,\n",
      "         -1.1992e+00, -1.1971e+00, -8.2943e-01,  1.3210e+00],\n",
      "        [-3.9092e-01,  6.1802e-01, -8.4900e-01, -1.0573e+00, -2.6247e-01,\n",
      "         -5.3152e-02, -1.0893e-02, -3.3186e-01,  1.8273e+00,  1.2947e+00,\n",
      "         -1.5788e-01, -1.6693e+00,  1.2245e+00, -2.3341e-01],\n",
      "        [ 1.5244e+00, -5.5875e-01,  1.1032e-01, -4.5667e-01, -2.3060e-01,\n",
      "          1.2796e+00,  7.3439e-04,  3.4217e-01,  7.2724e-01,  1.6311e+00,\n",
      "          2.0363e+00, -1.1800e+00, -3.6001e-01,  1.7112e+00],\n",
      "        [ 1.3118e-01, -3.9229e-01,  4.4631e-01, -4.0805e-01, -5.9314e-01,\n",
      "          1.5265e-01,  9.7037e-01, -4.0161e-01,  6.4738e-01,  5.5062e-01,\n",
      "          4.6917e-01, -8.0510e-01, -1.2872e-01,  1.6895e-01],\n",
      "        [ 3.9844e-01, -9.5003e-01,  9.0417e-02,  1.0769e+00,  4.1616e-01,\n",
      "         -2.4921e-01, -4.8947e-01, -1.2302e+00,  1.0626e+00,  9.1182e-01,\n",
      "         -9.8097e-01, -9.5540e-01, -3.4123e-01, -3.9292e-01],\n",
      "        [-6.5603e-01,  5.8817e-01,  2.9978e-01,  1.0334e+00, -5.3868e-01,\n",
      "         -4.9475e-01,  7.7762e-01, -4.1642e-03, -3.3837e-01,  1.1188e-01,\n",
      "         -9.3070e-01, -2.2128e-01, -5.3936e-01,  4.7503e-01],\n",
      "        [ 2.5778e-01,  8.0167e-01, -8.4647e-02,  5.6231e-01, -3.4730e-01,\n",
      "          6.9285e-01,  9.8632e-02,  8.4294e-02,  1.0003e+00, -4.8292e-01,\n",
      "         -1.5783e-01, -7.9135e-01, -3.5823e-01,  9.2239e-01],\n",
      "        [ 1.9433e-01,  3.8162e-01,  6.6717e-01, -1.8555e-01, -1.1348e+00,\n",
      "          3.2349e-01, -6.8880e-02,  1.5310e-01,  4.8827e-01,  8.5718e-01,\n",
      "          6.0395e-01, -6.8391e-01, -1.0881e+00,  9.5312e-02],\n",
      "        [-4.2478e-01, -4.2361e-01, -8.1150e-02, -4.6590e-01, -1.2930e+00,\n",
      "          7.8700e-01, -1.5271e-01,  3.5279e-01,  4.2757e-01,  6.4389e-01,\n",
      "          8.0959e-01,  1.3358e-02, -1.3169e+00,  5.6322e-01],\n",
      "        [-1.1914e-01, -3.8612e-01, -2.6878e-01, -4.6174e-02,  2.7447e-01,\n",
      "          5.0074e-01,  4.2765e-01,  4.7054e-01,  2.1707e-01,  3.0997e-01,\n",
      "          6.9932e-01, -1.4583e+00, -3.6946e-01, -1.7980e-01],\n",
      "        [ 8.1966e-01, -1.0467e+00,  1.6033e-01,  4.6485e-01, -1.2310e+00,\n",
      "         -3.4891e-02,  3.9968e-01, -8.3397e-03, -1.1835e-01,  7.2722e-01,\n",
      "          8.8305e-01,  5.7370e-01, -1.2877e+00,  1.4288e+00],\n",
      "        [ 1.6362e-01, -3.3640e-01,  3.8185e-02, -1.4495e-01, -2.1957e-01,\n",
      "         -2.3281e-01,  6.4747e-01,  1.3445e-01,  9.6155e-01,  1.2939e-01,\n",
      "         -7.2046e-03,  9.0643e-02, -8.1406e-02,  1.8573e-01],\n",
      "        [-2.6539e-01, -4.4733e-01, -3.4793e-01,  1.0193e+00, -2.2769e-01,\n",
      "          6.7700e-02, -4.6421e-01, -2.7457e-02,  5.6538e-01,  3.1202e-01,\n",
      "         -5.1006e-01, -5.3037e-01, -4.4980e-01,  1.0610e-01],\n",
      "        [ 1.1004e+00, -2.2405e-01,  2.5219e-01,  5.7100e-02,  8.2894e-01,\n",
      "          6.6902e-01, -2.0750e-01,  2.6914e-01,  1.2463e-01,  1.8819e-01,\n",
      "         -1.2675e+00, -8.5411e-02, -2.7848e-01,  1.5801e+00],\n",
      "        [ 4.9700e-02, -4.9340e-02, -1.6134e-02, -2.5655e-01,  3.5812e-01,\n",
      "          3.2704e-01,  4.4945e-01,  2.5583e-01,  7.4811e-01,  9.1393e-02,\n",
      "          1.2905e-01, -7.9256e-01,  6.2573e-01,  7.6932e-01],\n",
      "        [ 4.1598e-01, -1.4566e-01, -2.0943e-01,  5.2767e-01,  3.4884e-01,\n",
      "         -6.7296e-02,  1.6300e-01,  5.5777e-01,  2.3099e-01,  1.8166e-01,\n",
      "         -2.1197e-01, -1.7842e-01, -1.3833e-01,  4.7023e-01]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>) <class 'torch.Tensor'>\n",
      "x2:  tensor([[0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]], device='cuda:0',\n",
      "       dtype=torch.float64) <class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-625494d31697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Train the model!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m trained_model, validation_history = train_model(model=model, dataloaders=dataloaders, criterion=criterion, optimizer=optimizer,\n\u001b[0;32m---> 16\u001b[0;31m             scheduler=scheduler, model_name=model_name, save_dir=save_dir, save_all_epochs=save_all_epochs, num_epochs=num_epochs)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-8dbdabdb46b1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, scheduler, model_name, save_dir, save_all_epochs, num_epochs)\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;31m# Get model outputs and calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0;31m# torch.max outputs the maximum value, and its index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1822\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1823\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1825\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'target'"
     ]
    }
   ],
   "source": [
    "# Initialize the model for this run\n",
    "\n",
    "model = Chest_Disease_Net(cnn_model_name = model_name, num_classes = num_classes, resume_from = resume_from)\n",
    "input_size = model.input_size\n",
    "dataloaders = get_dataloaders(input_size, batch_size, shuffle_datasets)\n",
    "criterion = get_loss()\n",
    "\n",
    "# Move the model to the gpu if needed\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = make_optimizer(model)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5,10],gamma=0.1)\n",
    "# Train the model!\n",
    "trained_model, validation_history = train_model(model=model, dataloaders=dataloaders, criterion=criterion, optimizer=optimizer,\n",
    "            scheduler=scheduler, model_name=model_name, save_dir=save_dir, save_all_epochs=save_all_epochs, num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
