{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.2.0\n",
      "Torchvision Version:  0.4.0a0+6b959ee\n",
      "Using GPU!\n"
     ]
    }
   ],
   "source": [
    "import custom_models\n",
    "#python packages\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm\n",
    "import gc\n",
    "import datetime\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from skimage import io\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "#torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU!\")\n",
    "else:\n",
    "    print(\"WARNING: Could not find GPU! Using CPU only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_CNN(model_name, num_classes, resume_from = None):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    # The model (nn.Module) to return\n",
    "    model_ft = None\n",
    "    # The input image is expected to be (input_size, input_size)\n",
    "    input_size = 0\n",
    "    \n",
    "    # You may NOT use pretrained models!! \n",
    "    use_pretrained = False\n",
    "    \n",
    "    # By default, all parameters will be trained (useful when you're starting from scratch)\n",
    "    # Within this function you can set .requires_grad = False for various parameters, if you\n",
    "    # don't want to learn them\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    elif model_name == \"vgg13\":\n",
    "        \"\"\" VGG13_bn\n",
    "        \"\"\"\n",
    "        model_ft = custom_models.vgg13_bn(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    elif model_name == \"vgg16\":\n",
    "        \"\"\" VGG16_bn\n",
    "        \"\"\"\n",
    "        model_ft = custom_models.vgg13_bn(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "        \n",
    "    elif model_name == \"vgg19\":\n",
    "        \"\"\" VGG19_bn\n",
    "        \"\"\"\n",
    "        model_ft = custom_models.vgg13_bn(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "        input_size = 224\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Invalid model name!\")\n",
    "    \n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Multimodal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chest_Disease_Net(nn.Module):\n",
    "    \"\"\"\n",
    "    fc1: number of neurons in the hidden fully connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, cnn_model_name, num_classes, num_multimodal_features=12, fc1_out=32, resume_from=None):\n",
    "        #num_classes = 14\n",
    "        #num_multimodal_features= 12\n",
    "        super(Chest_Disease_Net, self).__init__()\n",
    "        self.cnn, self.input_size = make_CNN(cnn_model_name, num_classes)#models.vgg11(pretrained=False, progress = True)\n",
    "        #define output layers\n",
    "        self.fc1 = nn.Linear(num_classes + num_multimodal_features, fc1_out) #takes in input of CNN and multimodal input\n",
    "        self.fc2 = nn.Linear(fc1_out, num_classes)\n",
    "        if resume_from is not None:\n",
    "            print(\"Loading weights from %s\" % resume_from)\n",
    "            self.load_state_dict(torch.load(resume_from))\n",
    "        \n",
    "    def forward(self, image, data):\n",
    "        x1 = self.cnn(image)\n",
    "        #print(\"x1\", x1.shape)\n",
    "        x2 = data\n",
    "        #print(\"x2\", x2.shape)\n",
    "        #print(\"x1: \", x1, type(x1))\n",
    "        #print(\"x2: \", x2, type(x2))\n",
    "        #x = torch.cat((x1, x2), dim=1)  \n",
    "        x = torch.cat((x1.float(), x2.float()), dim=1) ### ???\n",
    "        #print(\"concat\", x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(\"relu\", x.shape)\n",
    "        x = self.fc2(x)\n",
    "       # print(\"fc2\", x.shape)\n",
    "        return x.double() ### ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset definition\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path, img_path, transform=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.img_path = img_path\n",
    "        self.transform = transform\n",
    "        self.diseases = self.get_diseases()\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        img_name = self.df.iloc[index][\"img_name\"] \n",
    "        img_path = os.path.join(self.img_path, img_name)\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = np.asarray(image)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor # ???\n",
    "        features = np.fromstring(self.df.iloc[index][\"feature\"][1:-1], sep=\",\") # ???\n",
    "        features = torch.from_numpy(features.astype(\"float\")) # ???\n",
    "        #label = int(self.df.iloc[index]['label'])\n",
    "        labels = list(self.df.iloc[index][self.diseases])\n",
    "        print(\"array labels: \", labels)\n",
    "        labels = torch.tensor(labels)\n",
    "        #print(\"Label type: \", type(label))\n",
    "        #label = np.int_(label) #???\n",
    "        #print(\"label type post casting: \", type(label))\n",
    "        return image, features, labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def get_diseases(self):\n",
    "        cols = list(self.df.columns)\n",
    "        cols.remove('disease')\n",
    "        cols.remove('feature')\n",
    "        cols.remove('img_name')\n",
    "        return cols\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(input_size, batch_size, num_classes, augment=False, shuffle = True):\n",
    "    # How to transform the image when you are loading them.\n",
    "    # you'll likely want to mess with the transforms on the training set.\n",
    "    \n",
    "    # For now, we resize/crop the image to the correct input size for our network,\n",
    "    # then convert it to a [C,H,W] tensor, then normalize it to values with a given mean/stdev. These normalization constants\n",
    "    # are derived from aggregating lots of data and happen to produce better results.\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            #Add extra transformations for data augmentation\n",
    "            transforms.RandomApply([\n",
    "                transforms.RandomChoice([\n",
    "                    transforms.RandomAffine(degrees=20),\n",
    "                    transforms.RandomAffine(degrees=0,scale=(0.1, 0.15)),\n",
    "                    transforms.RandomAffine(degrees=0,translate=(0.2,0.2)),\n",
    "                    #transforms.RandomAffine(degrees=0,shear=0.15),\n",
    "                    transforms.RandomHorizontalFlip(p=1.0)\n",
    "                ] if augment else [transforms.RandomAffine(degrees=0)])#else do nothing\n",
    "            ], p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.225])\n",
    "        ])\n",
    "    }\n",
    "    # Create training and validation datasets\n",
    "    data_subsets = {x: MultimodalDataset(csv_path=\"./data/\"+x+\"_dataset{}.csv\".format(num_classes), \n",
    "                                         img_path=\"/storage/images\", \n",
    "                                         transform=data_transforms[x]) for x in data_transforms.keys()}\n",
    "    # Create training and validation dataloaders\n",
    "    # Never shuffle the test set\n",
    "    dataloaders_dict = {x: DataLoader(data_subsets[x], batch_size=batch_size, shuffle=False if x != 'train' else shuffle, num_workers=4) for x in data_transforms.keys()}\n",
    "    return dataloaders_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, model_name=str(datetime.datetime.now()), \n",
    "                save_dir = None, save_all_epochs=False, num_epochs=25):\n",
    "    '''\n",
    "    model: The NN to train\n",
    "    dataloaders: A dictionary containing at least the keys \n",
    "                 'train','val' that maps to Pytorch data loaders for the dataset\n",
    "    criterion: The Loss function\n",
    "    optimizer: The algorithm to update weights \n",
    "               (Variations on gradient descent)\n",
    "    num_epochs: How many epochs to train for\n",
    "    save_dir: Where to save the best model weights that are found, \n",
    "              as they are found. Will save to save_dir/weights_best.pt\n",
    "              Using None will not write anything to disk\n",
    "    save_all_epochs: Whether to save weights for ALL epochs, not just the best\n",
    "                     validation error epoch. Will save to save_dir/weights_e{#}.pt\n",
    "    '''\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            # TQDM has nice progress bars\n",
    "            for inputs, features, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                features = features.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs, features)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # torch.max outputs the maximum value, and its index\n",
    "                    # Since the input is batched, we take the max along axis 1\n",
    "                    # (the meaningful outputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backprop + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), save_dir + \"/{}_best_weights.pt\".format(model_name))\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "        print()\n",
    "        scheduler.step()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(model):\n",
    "    # Get all the parameters\n",
    "    params_to_update = model.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "    # Use SGD\n",
    "    optimizer = optim.SGD(params_to_update, lr=0.01, momentum=0.9)\n",
    "    return optimizer\n",
    "\n",
    "def get_loss(num_classes):\n",
    "    # Create an instance of the loss function\n",
    "    #set weights to account for unbalanced data.\n",
    "    #In expectation every category class contributes the same to the loss\n",
    "    weights = torch.tensor([21.741, 9.533, 11.385,42.205,3.034,5.222, 1.000, 10.44]) if num_classes == 8 \\\n",
    "    else torch.tensor([])\n",
    "    #set weights[non_diseasd_class]  = 1/7\n",
    "    criterion = nn.BCEWithLogitsLoss(weights )\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet]\n",
    "# You can add your own, or modify these however you wish!\n",
    "model_name = \"vgg\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "# Miniplaces has 100\n",
    "num_classes = 8# set to 8 or 14\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "# You should use a power of 2.\n",
    "batch_size = 64\n",
    "\n",
    "# Shuffle the input data?\n",
    "shuffle_datasets = True\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 10\n",
    "\n",
    "### IO\n",
    "# Path to a model file to use to start weights at\n",
    "#resume_from = \"/home/ubuntu/6.867-xray-project/weights/data_aug_vgg.pt\"\n",
    "resume_from = None\n",
    "\n",
    "# Directory to save weights to\n",
    "save_dir = \"weights\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# If True saves the weights for all epochs, else only saves the weight of best one\n",
    "save_all_epochs = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t cnn.features.0.weight\n",
      "\t cnn.features.0.bias\n",
      "\t cnn.features.1.weight\n",
      "\t cnn.features.1.bias\n",
      "\t cnn.features.4.weight\n",
      "\t cnn.features.4.bias\n",
      "\t cnn.features.5.weight\n",
      "\t cnn.features.5.bias\n",
      "\t cnn.features.8.weight\n",
      "\t cnn.features.8.bias\n",
      "\t cnn.features.9.weight\n",
      "\t cnn.features.9.bias\n",
      "\t cnn.features.11.weight\n",
      "\t cnn.features.11.bias\n",
      "\t cnn.features.12.weight\n",
      "\t cnn.features.12.bias\n",
      "\t cnn.features.15.weight\n",
      "\t cnn.features.15.bias\n",
      "\t cnn.features.16.weight\n",
      "\t cnn.features.16.bias\n",
      "\t cnn.features.18.weight\n",
      "\t cnn.features.18.bias\n",
      "\t cnn.features.19.weight\n",
      "\t cnn.features.19.bias\n",
      "\t cnn.features.22.weight\n",
      "\t cnn.features.22.bias\n",
      "\t cnn.features.23.weight\n",
      "\t cnn.features.23.bias\n",
      "\t cnn.features.25.weight\n",
      "\t cnn.features.25.bias\n",
      "\t cnn.features.26.weight\n",
      "\t cnn.features.26.bias\n",
      "\t cnn.classifier.0.weight\n",
      "\t cnn.classifier.0.bias\n",
      "\t cnn.classifier.3.weight\n",
      "\t cnn.classifier.3.bias\n",
      "\t cnn.classifier.6.weight\n",
      "\t cnn.classifier.6.bias\n",
      "\t fc1.weight\n",
      "\t fc1.bias\n",
      "\t fc2.weight\n",
      "\t fc2.bias\n",
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f73f6333bf4429b118cfb3367f42e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1005), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f3ee5973828>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f3ee5973828>>\n",
      "    self._shutdown_workers()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f3ee5973828>>\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f3ee5973828>>\n",
      "Traceback (most recent call last):\n",
      "    w.join()\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    w.join()\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "    w.join()\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "AssertionError: can only join a child process\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "AssertionError: can only join a child process\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array labels:  [99089, 'train', 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
      "array labels:  [68853, 'train', 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "array labels:  [81990, 'train', 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "array labels:  [48581, 'train', 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "array labels:  [36477, 'train', 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "array labels:  [47265, 'train', 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "array labels:  [3106, 'train', 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "array labels:  [7994, 'train', 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "array labels:  [19420, 'train', 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-79-ea631625d485>\", line 29, in __getitem__\n    labels = torch.tensor(labels)\nTypeError: new(): invalid data type 'str'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-244890c3911e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Train the model!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m trained_model, validation_history = train_model(model=model, dataloaders=dataloaders, criterion=criterion, optimizer=optimizer,\n\u001b[0;32m---> 16\u001b[0;31m             scheduler=scheduler, model_name=model_name, save_dir=save_dir, save_all_epochs=save_all_epochs, num_epochs=num_epochs)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-8dbdabdb46b1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, scheduler, model_name, save_dir, save_all_epochs, num_epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# TQDM has nice progress bars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-79-ea631625d485>\", line 29, in __getitem__\n    labels = torch.tensor(labels)\nTypeError: new(): invalid data type 'str'\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for this run\n",
    "\n",
    "model = Chest_Disease_Net(cnn_model_name = model_name, num_classes = num_classes, resume_from = resume_from)\n",
    "input_size = model.input_size\n",
    "dataloaders = get_dataloaders(input_size, batch_size, num_classes, shuffle_datasets)\n",
    "criterion = get_loss(num_classes=num_classes)\n",
    "\n",
    "# Move the model to the gpu if needed\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = make_optimizer(model)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5,10],gamma=0.1)\n",
    "# Train the model!\n",
    "trained_model, validation_history = train_model(model=model, dataloaders=dataloaders, criterion=criterion, optimizer=optimizer,\n",
    "            scheduler=scheduler, model_name=model_name, save_dir=save_dir, save_all_epochs=save_all_epochs, num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
